{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed0926ea-63bb-4325-9a7c-7703fdc8420a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2d65955-86e0-4a84-9168-a29125d6a2a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 시드값 고정\n",
    "seed = 9\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae1ad2ce-dc32-4f23-9783-643b2ea070c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def age_map(x: int) -> int:\n",
    "    x = int(x)\n",
    "    if x < 20:\n",
    "        return 1\n",
    "    elif x >= 20 and x < 30:\n",
    "        return 2\n",
    "    elif x >= 30 and x < 40:\n",
    "        return 3\n",
    "    elif x >= 40 and x < 50:\n",
    "        return 4\n",
    "    elif x >= 50 and x < 60:\n",
    "        return 5\n",
    "    else:\n",
    "        return 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bde742a6-edb0-44e7-806c-3943afaaa834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_processing(train,test,sub,book,user):\n",
    "    ids = pd.concat([train['user_id'], sub['user_id']]).unique()\n",
    "    isbns = pd.concat([train['isbn'], sub['isbn']]).unique()\n",
    "\n",
    "    idx2user = {idx:id for idx, id in enumerate(ids)}\n",
    "    idx2isbn = {idx:isbn for idx, isbn in enumerate(isbns)}\n",
    "\n",
    "    user2idx = {id:idx for idx, id in idx2user.items()}\n",
    "    isbn2idx = {isbn:idx for idx, isbn in idx2isbn.items()}\n",
    "\n",
    "    train['user_id'] = train['user_id'].map(user2idx)\n",
    "    sub['user_id'] = sub['user_id'].map(user2idx)\n",
    "    test['user_id'] = test['user_id'].map(user2idx)\n",
    "    user['user_id'] = user['user_id'].map(user2idx)\n",
    "\n",
    "    train['isbn'] = train['isbn'].map(isbn2idx)\n",
    "    sub['isbn'] = sub['isbn'].map(isbn2idx)\n",
    "    test['isbn'] = test['isbn'].map(isbn2idx)\n",
    "    book['isbn'] = book['isbn'].map(isbn2idx)\n",
    "\n",
    "    ### 지역 설정\n",
    "    # 지역의 경우 세세한 지역보다는 간단한 국가 정도만 사용\n",
    "    user['location_city'] = user['location'].apply(lambda x: x.split(',')[0])\n",
    "    user['location_state'] = user['location'].apply(lambda x: x.split(',')[1])\n",
    "    user['location_country'] = user['location'].apply(lambda x: x.split(',')[2])\n",
    "    user = user.drop(['location'], axis=1)\n",
    "\n",
    "\n",
    "    ### 각 train 및 test별로 라벨 인덱싱이 필요한 경우 ~ data leakage 고려되야하는 부분 ~ 인덱싱 및 나이 통계치 확인                            \n",
    "    ratings = pd.concat([train, test]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    context_df = ratings.merge(user, on='user_id', how='left').merge(book[['isbn', 'category', 'publisher', 'language', 'book_author']], on='isbn', how='left')\n",
    "    train_df = train.merge(user, on='user_id', how='left').merge(book[['isbn', 'category', 'publisher', 'language', 'book_author']], on='isbn', how='left')\n",
    "    test_df = test.merge(user, on='user_id', how='left').merge(book[['isbn', 'category', 'publisher', 'language', 'book_author']], on='isbn', how='left')\n",
    "\n",
    "    loc_city2idx = {v:k for k,v in enumerate(context_df['location_city'].unique())}\n",
    "    loc_state2idx = {v:k for k,v in enumerate(context_df['location_state'].unique())}\n",
    "    loc_country2idx = {v:k for k,v in enumerate(context_df['location_country'].unique())}\n",
    "\n",
    "    train_df['location_city'] = train_df['location_city'].map(loc_city2idx)\n",
    "    train_df['location_state'] = train_df['location_state'].map(loc_state2idx)\n",
    "    train_df['location_country'] = train_df['location_country'].map(loc_country2idx)\n",
    "    test_df['location_city'] = test_df['location_city'].map(loc_city2idx)\n",
    "    test_df['location_state'] = test_df['location_state'].map(loc_state2idx)\n",
    "    test_df['location_country'] = test_df['location_country'].map(loc_country2idx)\n",
    "\n",
    "    train_df['age'] = train_df['age'].fillna(int(train_df['age'].mean()))\n",
    "    train_df['age'] = train_df['age'].apply(age_map)\n",
    "    test_df['age'] = test_df['age'].fillna(int(test_df['age'].mean()))\n",
    "    test_df['age'] = test_df['age'].apply(age_map)\n",
    "\n",
    "    # book 파트 인덱싱\n",
    "    category2idx = {v:k for k,v in enumerate(context_df['category'].unique())}\n",
    "    publisher2idx = {v:k for k,v in enumerate(context_df['publisher'].unique())}\n",
    "    language2idx = {v:k for k,v in enumerate(context_df['language'].unique())}\n",
    "    author2idx = {v:k for k,v in enumerate(context_df['book_author'].unique())}\n",
    "\n",
    "    train_df['category'] = train_df['category'].map(category2idx)\n",
    "    train_df['publisher'] = train_df['publisher'].map(publisher2idx)\n",
    "    train_df['language'] = train_df['language'].map(language2idx)\n",
    "    train_df['book_author'] = train_df['book_author'].map(author2idx)\n",
    "    test_df['category'] = test_df['category'].map(category2idx)\n",
    "    test_df['publisher'] = test_df['publisher'].map(publisher2idx)\n",
    "    test_df['language'] = test_df['language'].map(language2idx)\n",
    "    test_df['book_author'] = test_df['book_author'].map(author2idx)\n",
    "\n",
    "    # 필드 차원 수 정해주기\n",
    "    field_dim = np.array([len(user2idx), len(isbn2idx),\n",
    "                            6, len('loc_city2idx'), len('loc_state2idx'), len('loc_country2idx'),\n",
    "                            len('category2idx'), len('publisher2idx'), len('language2idx'), len('author2idx')], dtype=np.uint32)\n",
    "\n",
    "        \n",
    "    # 나중에 인덱싱한거 다시 되돌리기 용 및 기타 데이터 다 저장해서 넘기기 ~ data['train'] 이런식으로 조회 및 타 데이터 추가 가능하게\n",
    "    data = {\n",
    "            'train' : train_df,\n",
    "            'test' : test_df.drop(['rating'], axis=1),\n",
    "            'user':user,\n",
    "            'book':book,\n",
    "            'sub':sub,\n",
    "            'idx2user':idx2user,\n",
    "            'idx2isbn':idx2isbn,\n",
    "            'user2idx':user2idx,\n",
    "            'isbn2idx':isbn2idx,  \n",
    "            'field_dim' : field_dim   \n",
    "            }\n",
    "\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6ba682a-9b1c-4825-98e2-e1f9de981902",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def context_data_split(data):\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "                                                        data['train'].drop(['rating'], axis=1),\n",
    "                                                        data['train']['rating'],\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=seed,\n",
    "                                                        shuffle=True\n",
    "                                                        )\n",
    "    data['X_train'], data['X_valid'], data['y_train'], data['y_valid'] = X_train, X_valid, y_train, y_valid\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3883933f-434c-4433-a4b3-7a1947db1425",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "book = pd.read_csv('../../data/books.csv')\n",
    "user = pd.read_csv('../../data/users.csv')\n",
    "train = pd.read_csv('../../data/train_ratings.csv')\n",
    "test = pd.read_csv('../../data/test_ratings.csv')\n",
    "sub = pd.read_csv('../../data/sample_submission.csv')\n",
    "\n",
    "def seed_all(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def rmse(real: list, predict: list) -> float:\n",
    "    pred = np.array(predict)\n",
    "    return np.sqrt(np.mean((real-pred) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fd97e1e-bcf0-400e-acf6-7ded1d26ea31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed_all(seed)\n",
    "data = context_data_split(data_processing(train,test,sub,book,user))\n",
    "model = RandomForestRegressor(random_state=seed, n_jobs=-1,verbose=1)\n",
    "# n_jobs = -1 : using all processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baebca8d-92cb-4ec7-bbde-55d66371d679",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.249901007433584\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor(n_estimators=300, random_state=seed, max_depth=25, min_samples_leaf=5, min_samples_split = 8, max_features=None)\n",
    "model.fit(data['X_train'], data['y_train'])\n",
    "rfr_preds = model.predict(data['X_valid'])\n",
    "rmse_score = rmse(data['y_valid'].tolist(),rfr_preds.tolist())\n",
    "print(rmse_score)\n",
    "sub['rating'] = model.predict(data['test'])\n",
    "\n",
    "now = time.localtime()\n",
    "now_date = time.strftime('%Y%m%d', now)\n",
    "now_hour = time.strftime('%X', now)\n",
    "save_time = now_date + '_' + now_hour.replace(':', '')\n",
    "sub.to_csv('{}_{}_{}.csv'.format(save_time,\"rfr\",rmse_score.round(5)), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7762da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dcdb66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_kfold(data,n):\n",
    "    skf = StratifiedKFold(n_splits= 5, shuffle=True, random_state=seed)\n",
    "    counts = 0\n",
    "    for train_index, valid_index in skf.split(data['train'].drop(['rating'], axis=1),data['train']['rating']):\n",
    "        if counts == n:\n",
    "            data['X_train'], data['y_train'] = data['train'].drop(['rating'], axis=1).loc[train_index], data['train']['rating'].loc[train_index]\n",
    "            data['X_valid'], data['y_valid'] = data['train'].drop(['rating'], axis=1).loc[valid_index], data['train']['rating'].loc[valid_index]\n",
    "            break\n",
    "        else:\n",
    "            counts += 1\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04507915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2439194877111883\n",
      "2.2441563941011733\n",
      "2.2476427842649374\n",
      "2.2445941265639764\n",
      "2.2506781478472004\n"
     ]
    }
   ],
   "source": [
    "### k-fold\n",
    "data = data_processing(train,test,sub,book,user)\n",
    "\n",
    "predicts_list = []\n",
    "rmse_list = []\n",
    "for i in range(5):\n",
    "    data = stratified_kfold(data,i)\n",
    "    model = RandomForestRegressor(n_estimators=300, random_state=seed, max_depth=25, min_samples_leaf=5, min_samples_split = 8, max_features=None)\n",
    "    model.fit(data['X_train'], data['y_train'])\n",
    "    rfr_preds = model.predict(data['X_valid'])\n",
    "    rmse_score = rmse(data['y_valid'].tolist(),rfr_preds.tolist())\n",
    "    print(rmse_score)\n",
    "    rmse_list.append(rmse_score)\n",
    "    predicts_list.append(model.predict(data['test']))\n",
    "\n",
    "sub['rating'] = np.mean(predicts_list, axis=0)\n",
    "\n",
    "now = time.localtime()\n",
    "now_date = time.strftime('%Y%m%d', now)\n",
    "now_hour = time.strftime('%X', now)\n",
    "save_time = now_date + '_' + now_hour.replace(':', '')\n",
    "sub.to_csv(path + '{}_{}_{}.csv'.format(save_time,\"RF\",(sum(rmse_list)/len(rmse_list)).round(5)), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
