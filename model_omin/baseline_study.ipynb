{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "107efc7d",
   "metadata": {},
   "source": [
    "막막한 모델 구현을 쉽게 이해하기 위해, baseline의 구조를 하나의 흐름으로 펴는 작업을 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4a8abf5-a628-4735-90f6-2ac44c300c85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc2999c4",
   "metadata": {},
   "source": [
    "다양한 모델을 위한 args들이 함수의 입력으로 계속 따라붙으면서, baseline의 구조가 복잡하게 느껴진다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b6a698a",
   "metadata": {},
   "source": [
    "우선 FM 모델 하나만의 흐름으로 확인하자.  \n",
    "모델 흐름 이해에는 도움되지 않는 args들을 미리 지정해둔다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58893eb0-9b05-47a1-b75d-239082af8fa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = '/opt/ml/data/'\n",
    "saved_model_path ='./saved_models'\n",
    "model='FM' # ['FM', 'FFM', 'NCF', 'WDN', 'DCN', 'CNN_FM', 'DeepCoNN']\n",
    "data_shuffle=True\n",
    "test_size=0.2\n",
    "seed=42\n",
    "use_best_model=True\n",
    "\n",
    "batch_size = 1024\n",
    "epochs = 10\n",
    "lr = 1e-3\n",
    "loss_fn = 'RMSE'\n",
    "optimizer = 'ADAM'\n",
    "weight_decay = 1e-6\n",
    "\n",
    "device = 'cuda'\n",
    "embed_dim = 16\n",
    "dropout = 0.2\n",
    "mlp_dims = (16,16)\n",
    "num_layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9ba26db-aae4-48b0-b122-d77c496e69f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def context_data_load(args):\n",
    "######################## DATA LOAD\n",
    "users = pd.read_csv(data_path + 'users.csv')\n",
    "books = pd.read_csv(data_path + 'books.csv')\n",
    "train = pd.read_csv(data_path + 'train_ratings.csv')\n",
    "test = pd.read_csv(data_path + 'test_ratings.csv')\n",
    "sub = pd.read_csv(data_path + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1d40c1c-f67f-4c22-bd1d-f86da74778c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68069,) (59803,)\n"
     ]
    }
   ],
   "source": [
    "# id와 isbn(책 번호)를 쉬운 index로 바꾸기 위해 따로 분리한다.\n",
    "ids = pd.concat([train['user_id'], sub['user_id']]).unique()\n",
    "isbns = pd.concat([train['isbn'], sub['isbn']]).unique()\n",
    "tids = train['user_id'].unique()\n",
    "print(ids.shape,tids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c07e116-3446-4b45-bb08-651c18cf84af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx2user = {idx:id for idx, id in enumerate(ids)}\n",
    "idx2isbn = {idx:isbn for idx, isbn in enumerate(isbns)}\n",
    "\n",
    "user2idx = {id:idx for idx, id in idx2user.items()}\n",
    "isbn2idx = {isbn:idx for idx, isbn in idx2isbn.items()}\n",
    "\n",
    "# id와 isbn을 간단한 index로 변환\n",
    "train['user_id'] = train['user_id'].map(user2idx)\n",
    "sub['user_id'] = sub['user_id'].map(user2idx)\n",
    "test['user_id'] = test['user_id'].map(user2idx)\n",
    "users['user_id'] = users['user_id'].map(user2idx)\n",
    "\n",
    "train['isbn'] = train['isbn'].map(isbn2idx)\n",
    "sub['isbn'] = sub['isbn'].map(isbn2idx)\n",
    "test['isbn'] = test['isbn'].map(isbn2idx)\n",
    "books['isbn'] = books['isbn'].map(isbn2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76449ae0-eb91-4a07-8037-e9228fe5a36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_map(x: int) -> int:\n",
    "    x = int(x)\n",
    "    if x < 20:\n",
    "        return 1\n",
    "    elif x >= 20 and x < 30:\n",
    "        return 2\n",
    "    elif x >= 30 and x < 40:\n",
    "        return 3\n",
    "    elif x >= 40 and x < 50:\n",
    "        return 4\n",
    "    elif x >= 50 and x < 60:\n",
    "        return 5\n",
    "    else:\n",
    "        return 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "677c9eec-6b39-422f-b7b6-b3f981553dec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(383494, 3)\n"
     ]
    }
   ],
   "source": [
    "# def process_context_data(users, books, ratings1, ratings2):\n",
    "# location 분리\n",
    "users['location_city'] = users['location'].apply(lambda x: x.split(',')[0])\n",
    "users['location_state'] = users['location'].apply(lambda x: x.split(',')[1])\n",
    "users['location_country'] = users['location'].apply(lambda x: x.split(',')[2])\n",
    "users = users.drop(['location'], axis=1)\n",
    "\n",
    "ratings = pd.concat([train, test]).reset_index(drop=True)\n",
    "print(ratings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02bbead6-88c8-4bd2-91dc-ef1510b195a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 인덱싱 처리된 데이터 조인\n",
    "context_df = ratings.merge(users, on='user_id', how='left').merge(books[['isbn', 'category', 'publisher', 'language', 'book_author']], on='isbn', how='left')\n",
    "train_df = train.merge(users, on='user_id', how='left').merge(books[['isbn', 'category', 'publisher', 'language', 'book_author']], on='isbn', how='left')\n",
    "test_df = test.merge(users, on='user_id', how='left').merge(books[['isbn', 'category', 'publisher', 'language', 'book_author']], on='isbn', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9e7fdc",
   "metadata": {},
   "source": [
    "user_id와 isbn이 간단한 index로 바뀐 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e12c0145-4cab-4dad-bddb-f06ec6e9286a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>isbn</th>\n",
       "      <th>rating</th>\n",
       "      <th>age</th>\n",
       "      <th>location_city</th>\n",
       "      <th>location_state</th>\n",
       "      <th>location_country</th>\n",
       "      <th>category</th>\n",
       "      <th>publisher</th>\n",
       "      <th>language</th>\n",
       "      <th>book_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>timmins</td>\n",
       "      <td>ontario</td>\n",
       "      <td>canada</td>\n",
       "      <td>['Actresses']</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>en</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>30.0</td>\n",
       "      <td>toronto</td>\n",
       "      <td>ontario</td>\n",
       "      <td>canada</td>\n",
       "      <td>['Actresses']</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>en</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kingston</td>\n",
       "      <td>ontario</td>\n",
       "      <td>canada</td>\n",
       "      <td>['Actresses']</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>en</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>comber</td>\n",
       "      <td>ontario</td>\n",
       "      <td>canada</td>\n",
       "      <td>['Actresses']</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>en</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>guelph</td>\n",
       "      <td>ontario</td>\n",
       "      <td>canada</td>\n",
       "      <td>['Actresses']</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>en</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306790</th>\n",
       "      <td>6313</td>\n",
       "      <td>129772</td>\n",
       "      <td>7</td>\n",
       "      <td>28.0</td>\n",
       "      <td>pismo beach</td>\n",
       "      <td>california</td>\n",
       "      <td>usa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Simon &amp; Schuster Audio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>David Gardner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306791</th>\n",
       "      <td>1879</td>\n",
       "      <td>129773</td>\n",
       "      <td>6</td>\n",
       "      <td>33.0</td>\n",
       "      <td>dallas</td>\n",
       "      <td>texas</td>\n",
       "      <td>usa</td>\n",
       "      <td>['Humor']</td>\n",
       "      <td>Pocket Books</td>\n",
       "      <td>en</td>\n",
       "      <td>P.J. O'Rourke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306792</th>\n",
       "      <td>1879</td>\n",
       "      <td>129774</td>\n",
       "      <td>7</td>\n",
       "      <td>33.0</td>\n",
       "      <td>dallas</td>\n",
       "      <td>texas</td>\n",
       "      <td>usa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lone Star Books</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Claude Dooley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306793</th>\n",
       "      <td>1879</td>\n",
       "      <td>129775</td>\n",
       "      <td>7</td>\n",
       "      <td>33.0</td>\n",
       "      <td>dallas</td>\n",
       "      <td>texas</td>\n",
       "      <td>usa</td>\n",
       "      <td>['Fiction']</td>\n",
       "      <td>Kqed Books</td>\n",
       "      <td>en</td>\n",
       "      <td>Jeremy Lloyd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306794</th>\n",
       "      <td>1879</td>\n",
       "      <td>129776</td>\n",
       "      <td>10</td>\n",
       "      <td>33.0</td>\n",
       "      <td>dallas</td>\n",
       "      <td>texas</td>\n",
       "      <td>usa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>American Map Corporation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mapsco</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>306795 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id    isbn  rating   age location_city location_state   \n",
       "0             0       0       4   NaN       timmins        ontario  \\\n",
       "1             1       0       7  30.0       toronto        ontario   \n",
       "2             2       0       8   NaN      kingston        ontario   \n",
       "3             3       0       8   NaN        comber        ontario   \n",
       "4             4       0       9   NaN        guelph        ontario   \n",
       "...         ...     ...     ...   ...           ...            ...   \n",
       "306790     6313  129772       7  28.0   pismo beach     california   \n",
       "306791     1879  129773       6  33.0        dallas          texas   \n",
       "306792     1879  129774       7  33.0        dallas          texas   \n",
       "306793     1879  129775       7  33.0        dallas          texas   \n",
       "306794     1879  129776      10  33.0        dallas          texas   \n",
       "\n",
       "       location_country       category                 publisher language   \n",
       "0                canada  ['Actresses']     HarperFlamingo Canada       en  \\\n",
       "1                canada  ['Actresses']     HarperFlamingo Canada       en   \n",
       "2                canada  ['Actresses']     HarperFlamingo Canada       en   \n",
       "3                canada  ['Actresses']     HarperFlamingo Canada       en   \n",
       "4                canada  ['Actresses']     HarperFlamingo Canada       en   \n",
       "...                 ...            ...                       ...      ...   \n",
       "306790              usa            NaN    Simon & Schuster Audio      NaN   \n",
       "306791              usa      ['Humor']              Pocket Books       en   \n",
       "306792              usa            NaN           Lone Star Books      NaN   \n",
       "306793              usa    ['Fiction']                Kqed Books       en   \n",
       "306794              usa            NaN  American Map Corporation      NaN   \n",
       "\n",
       "                 book_author  \n",
       "0       Richard Bruce Wright  \n",
       "1       Richard Bruce Wright  \n",
       "2       Richard Bruce Wright  \n",
       "3       Richard Bruce Wright  \n",
       "4       Richard Bruce Wright  \n",
       "...                      ...  \n",
       "306790         David Gardner  \n",
       "306791         P.J. O'Rourke  \n",
       "306792         Claude Dooley  \n",
       "306793          Jeremy Lloyd  \n",
       "306794                Mapsco  \n",
       "\n",
       "[306795 rows x 11 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "279fc80b-e7de-42e7-825a-b26494598778",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 이제 나머지 인덱스들도 숫자로 변환하는 작업을 수행한다.\n",
    "loc_city2idx = {v:k for k,v in enumerate(context_df['location_city'].unique())}\n",
    "loc_state2idx = {v:k for k,v in enumerate(context_df['location_state'].unique())}\n",
    "loc_country2idx = {v:k for k,v in enumerate(context_df['location_country'].unique())}\n",
    "\n",
    "train_df['location_city'] = train_df['location_city'].map(loc_city2idx)\n",
    "train_df['location_state'] = train_df['location_state'].map(loc_state2idx)\n",
    "train_df['location_country'] = train_df['location_country'].map(loc_country2idx)\n",
    "test_df['location_city'] = test_df['location_city'].map(loc_city2idx)\n",
    "test_df['location_state'] = test_df['location_state'].map(loc_state2idx)\n",
    "test_df['location_country'] = test_df['location_country'].map(loc_country2idx)\n",
    "\n",
    "train_df['age'] = train_df['age'].fillna(int(train_df['age'].mean()))\n",
    "train_df['age'] = train_df['age'].apply(age_map)\n",
    "test_df['age'] = test_df['age'].fillna(int(test_df['age'].mean()))\n",
    "test_df['age'] = test_df['age'].apply(age_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b206fc4f-18f9-47bb-9d3b-e6399b88086a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# book 파트 인덱싱\n",
    "category2idx = {v:k for k,v in enumerate(context_df['category'].unique())}\n",
    "publisher2idx = {v:k for k,v in enumerate(context_df['publisher'].unique())}\n",
    "language2idx = {v:k for k,v in enumerate(context_df['language'].unique())}\n",
    "author2idx = {v:k for k,v in enumerate(context_df['book_author'].unique())}\n",
    "\n",
    "train_df['category'] = train_df['category'].map(category2idx)\n",
    "train_df['publisher'] = train_df['publisher'].map(publisher2idx)\n",
    "train_df['language'] = train_df['language'].map(language2idx)\n",
    "train_df['book_author'] = train_df['book_author'].map(author2idx)\n",
    "test_df['category'] = test_df['category'].map(category2idx)\n",
    "test_df['publisher'] = test_df['publisher'].map(publisher2idx)\n",
    "test_df['language'] = test_df['language'].map(language2idx)\n",
    "test_df['book_author'] = test_df['book_author'].map(author2idx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df4bde3f",
   "metadata": {},
   "source": [
    "모든 데이터가 숫자로 바뀐 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "201e1346-8f06-4633-8180-4e55c1037cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>isbn</th>\n",
       "      <th>rating</th>\n",
       "      <th>age</th>\n",
       "      <th>location_city</th>\n",
       "      <th>location_state</th>\n",
       "      <th>location_country</th>\n",
       "      <th>category</th>\n",
       "      <th>publisher</th>\n",
       "      <th>language</th>\n",
       "      <th>book_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306790</th>\n",
       "      <td>6313</td>\n",
       "      <td>129772</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1607</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2171</td>\n",
       "      <td>1</td>\n",
       "      <td>1272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306791</th>\n",
       "      <td>1879</td>\n",
       "      <td>129773</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>222</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306792</th>\n",
       "      <td>1879</td>\n",
       "      <td>129774</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10406</td>\n",
       "      <td>1</td>\n",
       "      <td>54713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306793</th>\n",
       "      <td>1879</td>\n",
       "      <td>129775</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6387</td>\n",
       "      <td>0</td>\n",
       "      <td>54714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306794</th>\n",
       "      <td>1879</td>\n",
       "      <td>129776</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10407</td>\n",
       "      <td>1</td>\n",
       "      <td>54715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>306795 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id    isbn  rating  age  location_city  location_state   \n",
       "0             0       0       4    3              0               0  \\\n",
       "1             1       0       7    3              1               0   \n",
       "2             2       0       8    3              2               0   \n",
       "3             3       0       8    3              3               0   \n",
       "4             4       0       9    3              4               0   \n",
       "...         ...     ...     ...  ...            ...             ...   \n",
       "306790     6313  129772       7    2           1607               5   \n",
       "306791     1879  129773       6    3             19              10   \n",
       "306792     1879  129774       7    3             19              10   \n",
       "306793     1879  129775       7    3             19              10   \n",
       "306794     1879  129776      10    3             19              10   \n",
       "\n",
       "        location_country  category  publisher  language  book_author  \n",
       "0                      0         0          0         0            0  \n",
       "1                      0         0          0         0            0  \n",
       "2                      0         0          0         0            0  \n",
       "3                      0         0          0         0            0  \n",
       "4                      0         0          0         0            0  \n",
       "...                  ...       ...        ...       ...          ...  \n",
       "306790                 1         5       2171         1         1272  \n",
       "306791                 1         7        222         0           69  \n",
       "306792                 1         5      10406         1        54713  \n",
       "306793                 1         3       6387         0        54714  \n",
       "306794                 1         5      10407         1        54715  \n",
       "\n",
       "[306795 rows x 11 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0d853fe-40e5-4074-970c-6183169553cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx = {\n",
    "    \"loc_city2idx\":loc_city2idx,\n",
    "    \"loc_state2idx\":loc_state2idx,\n",
    "    \"loc_country2idx\":loc_country2idx,\n",
    "    \"category2idx\":category2idx,\n",
    "    \"publisher2idx\":publisher2idx,\n",
    "    \"language2idx\":language2idx,\n",
    "    \"author2idx\":author2idx,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd491a98-1a2f-4efb-abb7-188a1a60ebce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "context_train, context_test = train_df, test_df\n",
    "field_dims = np.array([len(user2idx), len(isbn2idx),\n",
    "                        6, len(idx['loc_city2idx']), len(idx['loc_state2idx']), len(idx['loc_country2idx']),\n",
    "                        len(idx['category2idx']), len(idx['publisher2idx']), len(idx['language2idx']), len(idx['author2idx'])], dtype=np.uint32)\n",
    "\n",
    "data = {\n",
    "        'train':context_train,\n",
    "        'test':context_test.drop(['rating'], axis=1),\n",
    "        'field_dims':field_dims,\n",
    "        'users':users,\n",
    "        'books':books,\n",
    "        'sub':sub,\n",
    "        'idx2user':idx2user,\n",
    "        'idx2isbn':idx2isbn,\n",
    "        'user2idx':user2idx,\n",
    "        'isbn2idx':isbn2idx,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "789c41df-964b-4f63-b8cd-99826457458a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 68069, 149570,      6,  12267,   1609,    347,   4293,  11571,\n",
       "           27,  62059], dtype=uint32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26b240cb-4158-430c-8a11-a99c48299bc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def context_data_split(args, data):\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "                                                    data['train'].drop(['rating'], axis=1),\n",
    "                                                    data['train']['rating'],\n",
    "                                                    test_size=test_size,\n",
    "                                                    random_state=seed,\n",
    "                                                    shuffle=True\n",
    "                                                    )\n",
    "data['X_train'], data['X_valid'], data['y_train'], data['y_valid'] = X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0c1fda6-b3b4-4b3e-a532-f6830df0a85d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def context_data_loader(args, data):\n",
    "train_dataset = TensorDataset(torch.LongTensor(data['X_train'].values), torch.LongTensor(data['y_train'].values))\n",
    "valid_dataset = TensorDataset(torch.LongTensor(data['X_valid'].values), torch.LongTensor(data['y_valid'].values))\n",
    "test_dataset = TensorDataset(torch.LongTensor(data['test'].values))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=data_shuffle)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=data_shuffle)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "data['train_dataloader'], data['valid_dataloader'], data['test_dataloader'] = train_dataloader, valid_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc32d704-c67b-4178-8723-38e5e3c54fc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Setting:\n",
    "    @staticmethod\n",
    "    def seed_everything(seed):\n",
    "        '''\n",
    "        [description]\n",
    "        seed 값을 고정시키는 함수입니다.\n",
    "\n",
    "        [arguments]\n",
    "        seed : seed 값\n",
    "        '''\n",
    "        random.seed(seed)\n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    def __init__(self):\n",
    "        now = time.localtime()\n",
    "        now_date = time.strftime('%Y%m%d', now)\n",
    "        now_hour = time.strftime('%X', now)\n",
    "        save_time = now_date + '_' + now_hour.replace(':', '')\n",
    "        self.save_time = save_time\n",
    "\n",
    "    def get_log_path(self, model):\n",
    "        '''\n",
    "        [description]\n",
    "        log file을 저장할 경로를 반환하는 함수입니다.\n",
    "\n",
    "        [arguments]\n",
    "        args : argparse로 입력받은 args 값으로 이를 통해 모델의 정보를 전달받습니다.\n",
    "\n",
    "        [return]\n",
    "        path : log file을 저장할 경로를 반환합니다.\n",
    "        이 때, 경로는 log/날짜_시간_모델명/ 입니다.\n",
    "        '''\n",
    "        path = f'./log/{self.save_time}_{model}/'\n",
    "        return path\n",
    "\n",
    "    def get_submit_filename(self, model):\n",
    "        '''\n",
    "        [description]\n",
    "        submit file을 저장할 경로를 반환하는 함수입니다.\n",
    "\n",
    "        [arguments]\n",
    "        args : argparse로 입력받은 args 값으로 이를 통해 모델의 정보를 전달받습니다.\n",
    "\n",
    "        [return]\n",
    "        filename : submit file을 저장할 경로를 반환합니다.\n",
    "        이 때, 파일명은 submit/날짜_시간_모델명.csv 입니다.\n",
    "        '''\n",
    "        path = self.make_dir(\"./submit/\")\n",
    "        filename = f'{path}{self.save_time}_{model}.csv'\n",
    "        return filename\n",
    "\n",
    "    def make_dir(self,path):\n",
    "        '''\n",
    "        [description]\n",
    "        경로가 존재하지 않을 경우 해당 경로를 생성하며, 존재할 경우 pass를 하는 함수입니다.\n",
    "\n",
    "        [arguments]\n",
    "        path : 경로\n",
    "\n",
    "        [return]\n",
    "        path : 경로\n",
    "        '''\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        else:\n",
    "            pass\n",
    "        return path\n",
    "\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self, model, path):\n",
    "        \"\"\"\n",
    "        [description]\n",
    "        log file을 생성하는 클래스입니다.\n",
    "\n",
    "        [arguments]\n",
    "        args : argparse로 입력받은 args 값으로 이를 통해 모델의 정보를 전달받습니다.\n",
    "        path : log file을 저장할 경로를 전달받습니다.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.path = path\n",
    "\n",
    "        self.logger = logging.getLogger()\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        self.formatter = logging.Formatter('[%(asctime)s] - %(message)s')\n",
    "\n",
    "        self.file_handler = logging.FileHandler(self.path+'train.log')\n",
    "        self.file_handler.setFormatter(self.formatter)\n",
    "        self.logger.addHandler(self.file_handler)\n",
    "\n",
    "    def log(self, epoch, train_loss, valid_loss):\n",
    "        '''\n",
    "        [description]\n",
    "        log file에 epoch, train loss, valid loss를 기록하는 함수입니다.\n",
    "        이 때, log file은 train.log로 저장됩니다.\n",
    "\n",
    "        [arguments]\n",
    "        epoch : epoch\n",
    "        train_loss : train loss\n",
    "        valid_loss : valid loss\n",
    "        '''\n",
    "        message = f'epoch : {epoch}/{epochs} | train loss : {train_loss:.3f} | valid loss : {valid_loss:.3f}'\n",
    "        self.logger.info(message)\n",
    "\n",
    "    def close(self):\n",
    "        '''\n",
    "        [description]\n",
    "        log file을 닫는 함수입니다.\n",
    "        '''\n",
    "        self.logger.removeHandler(self.file_handler)\n",
    "        self.file_handler.close()\n",
    "\n",
    "# args는 초반에 다 지정해버렸기 때문에, 이 함수는 필요없다.\n",
    "#     def save_args(self):\n",
    "#         '''\n",
    "#         [description]\n",
    "#         model에 사용된 args를 저장하는 함수입니다.\n",
    "#         이 때, 저장되는 파일명은 model.json으로 저장됩니다.\n",
    "#         '''\n",
    "#         argparse_dict = self.args.__dict__\n",
    "\n",
    "#         with open(f'{self.path}/model.json', 'w') as f:\n",
    "#             json.dump(argparse_dict,f,indent=4)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de6a8203-5f29-4d53-9031-cf5da749adbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "setting = Setting()\n",
    "\n",
    "log_path = setting.get_log_path(model)\n",
    "setting.make_dir(log_path)\n",
    "\n",
    "logger = Logger(model, log_path)\n",
    "# logger.save_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54a1ff69-db0c-4b13-bd09-cc64e5390868",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# factorization을 통해 얻은 feature를 embedding 합니다.\n",
    "class FeaturesEmbedding(nn.Module):\n",
    "    def __init__(self, field_dims: np.ndarray, embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(sum(field_dims), embed_dim)\n",
    "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.int32)\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight.data)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
    "        return self.embedding(x)\n",
    "\n",
    "\n",
    "# FM모델 등에서 활용되는 선형 결합 부분을 정의합니다.\n",
    "class FeaturesLinear(nn.Module):\n",
    "    def __init__(self, field_dims: np.ndarray, output_dim: int=1):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Embedding(sum(field_dims), output_dim)\n",
    "        self.bias = torch.nn.Parameter(torch.zeros((output_dim,)))\n",
    "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.int32)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
    "        return torch.sum(self.fc(x), dim=1) + self.bias\n",
    "\n",
    "\n",
    "# feature 사이의 상호작용을 효율적으로 계산합니다.\n",
    "class FactorizationMachine(nn.Module):\n",
    "    def __init__(self, reduce_sum:bool=True):\n",
    "        super().__init__()\n",
    "        self.reduce_sum = reduce_sum\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        square_of_sum = torch.sum(x, dim=1) ** 2\n",
    "        sum_of_square = torch.sum(x ** 2, dim=1)\n",
    "        ix = square_of_sum - sum_of_square\n",
    "        if self.reduce_sum:\n",
    "            ix = torch.sum(ix, dim=1, keepdim=True)\n",
    "        return 0.5 * ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a705a92b-1f70-4d26-856e-bd84f354e513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizationMachineModel(nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.field_dims = data['field_dims']\n",
    "        self.embedding = FeaturesEmbedding(self.field_dims, embed_dim)\n",
    "        self.linear = FeaturesLinear(self.field_dims)\n",
    "        self.fm = FactorizationMachine(reduce_sum=True)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.linear(x) + self.fm(self.embedding(x))\n",
    "        # return torch.sigmoid(x.squeeze(1))\n",
    "        return x.squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dff18fb5-0c3e-4773-95ae-8ff64c95c2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def models_load(args, data):\n",
    "model = FactorizationMachineModel(data).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddd32b0a-953b-4119-9027-f6b0fa68a31e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss, self).__init__()\n",
    "        self.eps = 1e-6\n",
    "    def forward(self, x, y):\n",
    "        criterion = MSELoss()\n",
    "        loss = torch.sqrt(criterion(x, y)+self.eps)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "528eadb8-728a-41bb-a908-feccce791763",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def valid(model, dataloader, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    batch = 0\n",
    "\n",
    "    for idx, data in enumerate(dataloader['valid_dataloader']):\n",
    "        x, y = data[0].to(device), data[1].to(device)\n",
    "        y_hat = model(x)\n",
    "        loss = loss_fn(y.float(), y_hat)\n",
    "        total_loss += loss.item()\n",
    "        batch +=1\n",
    "    valid_loss = total_loss/batch\n",
    "    return valid_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80c7ff98-a259-4a58-8598-9a2bceb0f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, logger, setting):\n",
    "    print(model)\n",
    "    minimum_loss = 999999999\n",
    "    loss_fn = RMSELoss()\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    for epoch in tqdm.tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        batch = 0\n",
    "\n",
    "        for idx, data in enumerate(dataloader['train_dataloader']):\n",
    "            x, y = data[0].to(device), data[1].to(device)\n",
    "            y_hat = model(x)\n",
    "            loss = loss_fn(y.float(), y_hat)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            batch +=1\n",
    "        valid_loss = valid(model, dataloader, loss_fn)\n",
    "        print(f'Epoch: {epoch+1}, Train_loss: {total_loss/batch:.3f}, valid_loss: {valid_loss:.3f}')\n",
    "        logger.log(epoch=epoch+1, train_loss=total_loss/batch, valid_loss=valid_loss)\n",
    "        if minimum_loss > valid_loss:\n",
    "            minimum_loss = valid_loss\n",
    "            os.makedirs(saved_model_path, exist_ok=True)\n",
    "            # torch.save(state_dict(), f'{saved_model_path}/{setting.save_time}_{model}_model.pt')\n",
    "    logger.close()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbcf041d-f3d9-40b0-aa29-6824e0ee3288",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FactorizationMachineModel(\n",
      "  (embedding): FeaturesEmbedding(\n",
      "    (embedding): Embedding(309818, 16)\n",
      "  )\n",
      "  (linear): FeaturesLinear(\n",
      "    (fc): Embedding(309818, 1)\n",
      "  )\n",
      "  (fm): FactorizationMachine()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:03<00:30,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train_loss: 5.387, valid_loss: 2.903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:06<00:27,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train_loss: 2.536, valid_loss: 2.585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:10<00:24,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train_loss: 2.136, valid_loss: 2.503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:13<00:20,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train_loss: 1.901, valid_loss: 2.477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:17<00:17,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train_loss: 1.742, valid_loss: 2.477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:20<00:13,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train_loss: 1.629, valid_loss: 2.487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:23<00:10,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train_loss: 1.548, valid_loss: 2.504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:27<00:06,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train_loss: 1.490, valid_loss: 2.522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:30<00:03,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train_loss: 1.447, valid_loss: 2.541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:33<00:00,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train_loss: 1.416, valid_loss: 2.557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = train(model, data, logger, setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d619f09-070c-4f57-8262-0a2daa3e779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, setting):\n",
    "    predicts = list()\n",
    "    # if use_best_model == True:\n",
    "    #     model.load_state_dict(torch.load(f'./saved_models/{setting.save_time}_{model}_model.pt'))\n",
    "    # else:\n",
    "    #     pass\n",
    "    model.eval()\n",
    "\n",
    "    for idx, data in enumerate(dataloader['test_dataloader']):\n",
    "        x = data[0].to(device)\n",
    "        y_hat = model(x)\n",
    "        predicts.extend(y_hat.tolist())\n",
    "    return predicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89d66f3c-e4de-4637-a089-25a356061baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- SAVE FactorizationMachineModel(\n",
      "  (embedding): FeaturesEmbedding(\n",
      "    (embedding): Embedding(309818, 16)\n",
      "  )\n",
      "  (linear): FeaturesLinear(\n",
      "    (fc): Embedding(309818, 1)\n",
      "  )\n",
      "  (fm): FactorizationMachine()\n",
      ") PREDICT ---------------\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './saved_models/20230416_122752_FactorizationMachineModel(\\n  (embedding): FeaturesEmbedding(\\n    (embedding): Embedding(309818, 16)\\n  )\\n  (linear): FeaturesLinear(\\n    (fc): Embedding(309818, 1)\\n  )\\n  (fm): FactorizationMachine()\\n)_model.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m--------------- SAVE \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m}\u001b[39;00m\u001b[39m PREDICT ---------------\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m submission \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(data_path \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39msample_submission.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m submission[\u001b[39m'\u001b[39m\u001b[39mrating\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m test(model, data, setting)\n\u001b[1;32m      5\u001b[0m submission\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mresult\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[27], line 4\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, dataloader, setting)\u001b[0m\n\u001b[1;32m      2\u001b[0m predicts \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m()\n\u001b[1;32m      3\u001b[0m \u001b[39mif\u001b[39;00m use_best_model \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m     model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./saved_models/\u001b[39;49m\u001b[39m{\u001b[39;49;00msetting\u001b[39m.\u001b[39;49msave_time\u001b[39m}\u001b[39;49;00m\u001b[39m_\u001b[39;49m\u001b[39m{\u001b[39;49;00mmodel\u001b[39m}\u001b[39;49;00m\u001b[39m_model.pt\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m      5\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py:581\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    579\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 581\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    582\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    583\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    584\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    585\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    586\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    231\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './saved_models/20230416_122752_FactorizationMachineModel(\\n  (embedding): FeaturesEmbedding(\\n    (embedding): Embedding(309818, 16)\\n  )\\n  (linear): FeaturesLinear(\\n    (fc): Embedding(309818, 1)\\n  )\\n  (fm): FactorizationMachine()\\n)_model.pt'"
     ]
    }
   ],
   "source": [
    "######################## SAVE PREDICT\n",
    "print(f'--------------- SAVE {model} PREDICT ---------------')\n",
    "submission = pd.read_csv(data_path + 'sample_submission.csv')\n",
    "submission['rating'] = test(model, data, setting)\n",
    "submission.to_csv('result', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
